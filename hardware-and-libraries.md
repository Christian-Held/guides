Hier ist deine kompakte **Zusammenfassung mit allen wichtigen Parametern**, strukturiert nach **Hardware, Bibliotheken und dem gesamten Fine-Tuning-Prozess**:

---

## **1Ô∏è‚É£ Hardware-Check**
| **Komponente** | **Befehl zur √úberpr√ºfung** | **Erwarteter Wert** |
|--------------|--------------------|------------------|
| **CPU** | `lscpu` | Mind. 6 Kerne f√ºr schnelles Training |
| **GPU** | `nvidia-smi` | RTX 2060 mit 8GB VRAM |
| **CUDA-Version** | `nvcc --version` | `CUDA 12.8` |
| **PyTorch Version** | `python3 -c "import torch; print(torch.__version__)"` | `>=2.1.0` |
| **BitsAndBytes verf√ºgbar?** | `python3 -c "import bitsandbytes"` | Kein Fehler |
| **RAM** | `free -h` | Mind. 16GB empfohlen |

‚ùå **Wichtiger Hinweis:** **Deine GPU unterst√ºtzt kein FlashAttention2**, daher wurde es entfernt!

---

## **2Ô∏è‚É£ Wichtige Bibliotheken & Versionen**
| **Bibliothek** | **Version pr√ºfen** | **Erwartete Version** |
|---------------|--------------------|--------------------|
| **CUDA** | `nvcc --version` | `12.8` |
| **PyTorch** | `python3 -c "import torch; print(torch.__version__)"` | `>=2.1.0` |
| **Transformers** | `python3 -c "import transformers; print(transformers.__version__)"` | `>=4.36` |
| **Datasets** | `python3 -c "import datasets; print(datasets.__version__)"` | `>=2.15` |
| **BitsAndBytes** | `python3 -c "import bitsandbytes"` | Muss ohne Fehler laufen |
| **PEFT (LoRA)** | `python3 -c "import peft; print(peft.__version__)"` | `>=0.5.0` |

Falls Versionen nicht stimmen:  
```bash
pip install torch transformers datasets bitsandbytes peft --upgrade
```

---

## **3Ô∏è‚É£ JSONL-Daten & Tokenisierung**
| **Schritt** | **Parameter** | **Speicherung** |
|------------|--------------|---------------|
| **Datenquelle** | `dataset_christian_held.jsonl` (1000 Samples) | JSONL |
| **Sprache** | **Nur Deutsch** (‚ùì **Problem?** ‚Üí Modell darauf trainieren?) | `language: "de"` |
| **Tokenisierung** | `AutoTokenizer` (DeepSeek-Qwen) | `tokenized_dataset_christian_held` |
| **Pr√ºfung** | `python3 check002.py` | Input-IDs, Padding, Labels pr√ºfen |
| **Padding** | `context_length=128`, `pad_token_id=151643` | `padded_dataset_christian_held` |

**Frage:** Willst du das Modell mehrsprachig trainieren oder nur auf Deutsch?

---

## **4Ô∏è‚É£ Padding & Speicherung**
| **Schritt** | **Parameter** | **Speicherung** |
|------------|--------------|---------------|
| **Padding auf 128** | `context_length=128` | `padded_dataset_christian_held` |
| **Padding-Tokens** | `pad_token_id=151643` (`<|end_of_sentence|>`) | `input_ids`, `labels`, `attention_mask` |
| **Speicherung** | `datasets.save_to_disk()` | `.arrow`-Format f√ºr Effizienz |

**Check:**  
```bash
python3 -c "from datasets import load_from_disk; dataset=load_from_disk('padded_dataset_christian_held'); print(dataset[0])"
```

---

## **5Ô∏è‚É£ Fine-Tuning mit LoRA**
| **Schritt** | **Parameter** | **Speicherung** |
|------------|--------------|---------------|
| **Modell** | `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` | Transformer-Modell |
| **Device** | `"cuda"` (RTX 2060) | GPU-Nutzung |
| **Quantisierung** | `8bit` | Reduziert Speicherverbrauch |
| **LoRA R** | `64` | Low-Rank Approximation |
| **LoRA Alpha** | `32` | Skalierungsfaktor |
| **Trainierbare Module** | `q_proj, v_proj, k_proj, o_proj, gate_proj` | PEFT |
| **Optimierung** | `adamw_torch` | Standard f√ºr LLMs |
| **Gradient Accumulation** | `4` | Akkumulierte Gradienten f√ºr bessere Effizienz |

**Trainingsstart f√ºr 8bit:**
```bash
python3 finetune_dataset_christian_held.py \
    --model_path "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B" \
    --tokenized_data "padded_dataset_christian_held" \
    --output_dir "/mnt/externe-platte/christian_held_finetuned_8bit" \
    --context_length 128 \
    --epochs 20 \
    --batch_size 4 \
    --device cuda \
    --quantize 8bit \
    --gradient_accumulation_steps 4 \
    --learning_rate 3e-5 \
    --warmup_ratio 0.1 \
    --save_steps 50 \
    --eval_steps 20 \
    --save_total_limit 3 \
    --num_workers 4
```

Falls **8bit nicht l√§uft**, probiere stattdessen **4bit**:
```bash
python3 finetune_dataset_christian_held.py --quantize 4bit
```

---

## **6Ô∏è‚É£ Training & Checkpoints**
| **Schritt** | **Parameter** | **Speicherung** |
|------------|--------------|---------------|
| **Epochs** | `20` | Anzahl vollst√§ndiger Trainingsdurchl√§ufe |
| **Batch Size** | `4` | Optimiert f√ºr 8GB VRAM |
| **Save Steps** | `50` | Speichert Checkpoints |
| **Eval Steps** | `20` | Evaluation w√§hrend Training |
| **Max Checkpoints** | `3` | Spart Speicherplatz |

**Check:**  
```bash
tail -f /mnt/externe-platte/christian_held_finetuned_8bit/checkpoint-100/trainer_state.json
```

---

## **7Ô∏è‚É£ Testen des Modells**
| **Schritt** | **Parameter** | **Speicherung** |
|------------|--------------|---------------|
| **Speicherung** | `adapter_model.safetensors` | LoRA-Adapter |
| **Finales Modell** | `AutoModelForCausalLM.from_pretrained()` | Laden f√ºr Inferenz |
| **Pipeline-Test** | `transformers.pipeline("text-generation")` | Textgenerierung |

```python
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

model_path = "/mnt/externe-platte/christian_held_finetuned_8bit"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device=0)
prompt = "Wer ist Christian Held?"
print(pipe(prompt, max_length=128, do_sample=True))
```

---

## **8Ô∏è‚É£ Fazit & Optimierungsm√∂glichkeiten**
| **Problem** | **L√∂sung** |
|------------|-----------|
| **Speichernutzung** | 8bit/4bit Quantisierung reduziert VRAM |
| **Lange Trainingszeit** | Gradient Accumulation reduziert VRAM-Last |
| **Nur Deutsch?** | Mehrsprachige JSONL-Daten nutzen |

Lass mich wissen, ob du **mehr optimieren** oder **neue Features testen** willst! üöÄ
