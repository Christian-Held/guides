## **ğŸ”¥ Fine-Tuning Command Breakdown**
Example command:
```bash
nohup python finetune_christian_held.py \
    --model_path "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B" \
    --train_data "dataset_christian_held_002.json" \
    --output_dir "/mnt/externe-platte/christian_held_finetuned_002" \
    --epochs 50 \
    --batch_size 1 \
    --gradient_accumulation_steps 16 \
    --context_length 256 \
    --quantize 8bit \
    --learning_rate 1e-5 \
    --warmup_ratio 0.2 \
    --device cuda \
    --eval > training_002.log 2>&1 &
```

### **ğŸ“Œ 1. `--model_path`**
ğŸ“¢ **What it does:**  
Defines **which base model** you are fine-tuning.  
âœ… **Example:** `"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"`  
â— **Trade-off:** Choosing a **bigger model (e.g., 7B or 13B)** **increases VRAM usage**.

ğŸ“Š **Best Practices:**  
- **Small GPUs (â‰¤ 8GB VRAM):** Use **1.5B models** with 8-bit/4-bit quantization.  
- **Larger GPUs (â‰¥ 24GB VRAM):** You can try **bigger models** like 7B or 13B.  

---

### **ğŸ“Œ 2. `--train_data`**
ğŸ“¢ **What it does:**  
Specifies the **training dataset file**.  
âœ… **Example:** `"dataset_christian_held_002.json"`  

ğŸ“Š **Best Practices:**  
- Use **.json** or **.jsonl** for structured data.  
- Make sure data **is tokenized and padded** properly.  

---

### **ğŸ“Œ 3. `--output_dir`**
ğŸ“¢ **What it does:**  
Defines where the **fine-tuned model** will be saved.  
âœ… **Example:** `"/mnt/externe-platte/christian_held_finetuned_002"`

ğŸ“Š **Trade-off:**  
- Ensure the directory **has enough storage space** (~10GB per checkpoint).  

---

### **ğŸ“Œ 4. `--epochs`**
ğŸ“¢ **What it does:**  
Defines **how many times** the entire dataset is processed.  
âœ… **Example:** `--epochs 50`  

ğŸ“Š **Effects:**  
- **Higher epochs (e.g., 100+)** â†’ **Better learning**, but overfitting risk.
- **Lower epochs (e.g., 10-20)** â†’ **Faster training**, but may **not generalize well**.

ğŸ“Š **Best Practices:**  
- **Start with 20-50 epochs** for small datasets.  
- **Use early stopping** if possible to **prevent overfitting**.  

---

### **ğŸ“Œ 5. `--batch_size`**
ğŸ“¢ **What it does:**  
Defines **how many examples** are processed at once.  
âœ… **Example:** `--batch_size 1`  

ğŸ“Š **Trade-offs:**  
- **Larger batch sizes (4, 8, 16+)** â†’ **Faster training**, but needs **more VRAM**.  
- **Smaller batch sizes (1, 2)** â†’ **Less VRAM usage**, but **slower learning**.  

ğŸ“Š **Best Practices:**  
- **If VRAM is low (â‰¤ 8GB):** Use **batch_size = 1** with **gradient accumulation**.  
- **If VRAM is high (â‰¥ 24GB):** Use batch_size **4+** for **faster convergence**.  

---

### **ğŸ“Œ 6. `--gradient_accumulation_steps`**
ğŸ“¢ **What it does:**  
Simulates **a larger batch size** by accumulating gradients.  
âœ… **Example:** `--gradient_accumulation_steps 16`  

ğŸ“Š **Effects:**  
- **Higher values (8-32)** â†’ Simulates **larger batch size**, but slower updates.  
- **Lower values (1-4)** â†’ Faster updates, but may **not generalize well**.  

ğŸ“Š **Best Practices:**  
- **For small VRAM:** Set **16 or higher**.  
- **For larger VRAM:** Keep at **4-8** for balance.  

---

### **ğŸ“Œ 7. `--context_length`**
ğŸ“¢ **What it does:**  
Defines the **maximum token length** per input.  
âœ… **Example:** `--context_length 256`  

ğŸ“Š **Trade-offs:**  
- **Higher values (512-1024)** â†’ Better understanding of **long text**, but **more VRAM usage**.  
- **Lower values (128-256)** â†’ Fits in **low VRAM**, but may **cut important context**.  

ğŸ“Š **Best Practices:**  
- **RTX 2060 (6-8GB VRAM):** Keep **â‰¤ 256 tokens**.  
- **RTX 3090+ (24GB VRAM):** Can use **512+ tokens**.  

---

### **ğŸ“Œ 8. `--quantize`**
ğŸ“¢ **What it does:**  
Reduces model precision to **save VRAM**.  
âœ… **Options:**  
- `"none"` â†’ No quantization (best quality, max VRAM usage).  
- `"8bit"` â†’ 8-bit precision (**50% less VRAM**, slight loss in quality).  
- `"4bit"` â†’ 4-bit precision (**75% less VRAM**, more quality loss).  

ğŸ“Š **Best Practices:**  
- **Low VRAM (â‰¤ 8GB):** Use `"8bit"` or `"4bit"`.  
- **If you want best accuracy:** Use `"none"`, but needs **big GPU**.  

---

### **ğŸ“Œ 9. `--learning_rate`**
ğŸ“¢ **What it does:**  
Defines **how fast** weights are updated.  
âœ… **Example:** `--learning_rate 1e-5`  

ğŸ“Š **Effects:**  
- **Higher (3e-5 - 1e-4)** â†’ Faster learning, but **risk of instability**.  
- **Lower (1e-5 - 3e-6)** â†’ More **stable**, but slower convergence.  

ğŸ“Š **Best Practices:**  
- **If dataset is small** â†’ Use **lower learning rate (1e-5)**.  
- **If dataset is large** â†’ Use **higher learning rate (3e-5)**.  

---

### **ğŸ“Œ 10. `--warmup_ratio`**
ğŸ“¢ **What it does:**  
Gradually increases learning rate at the start.  
âœ… **Example:** `--warmup_ratio 0.2`  

ğŸ“Š **Effects:**  
- **Higher (0.2 - 0.3)** â†’ **More stable training** (less chance of exploding gradients).  
- **Lower (0.05 - 0.1)** â†’ Faster adaptation, but **risk of bad initialization**.  

ğŸ“Š **Best Practices:**  
- **For small datasets:** Use **0.1-0.2**.  
- **For large datasets:** Use **0.05**.  

---

### **ğŸ“Œ 11. `--device`**
ğŸ“¢ **What it does:**  
Specifies where to run training.  
âœ… **Options:** `"cuda"` or `"cpu"`  

ğŸ“Š **Trade-offs:**  
- **CUDA (GPU)** â†’ **Faster**, but needs **VRAM**.  
- **CPU** â†’ **Much slower**, but can run on any system.  

ğŸ“Š **Best Practices:**  
- Always use **CUDA** if you have a **GPU**.  

---

### **ğŸ“Œ 12. `--eval`**
ğŸ“¢ **What it does:**  
Runs **evaluation** during training.  
âœ… **Example:** `--eval`  

ğŸ“Š **Trade-offs:**  
- **Enabled (`--eval`)** â†’ Checks validation loss, but **needs more compute**.  
- **Disabled (remove `--eval`)** â†’ Faster training, but **no validation feedback**.  

ğŸ“Š **Best Practices:**  
- Use `--eval` for better monitoring.  

---

## **ğŸš€ Conclusion**
| **Argument**        | **Effect**                   | **Trade-Offs** |
|---------------------|----------------------------|----------------|
| `--batch_size 1`   | Uses **less VRAM**, slow | Slower training |
| `--gradient_accumulation_steps 16` | Simulates **larger batch**, better learning | Slower updates |
| `--context_length 256` | Smaller VRAM usage | May cut important context |
| `--quantize 8bit` | Saves **50% VRAM** | Slight quality loss |

Hope this helps! ğŸš€ğŸ”¥
